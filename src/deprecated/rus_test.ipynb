{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from scipy.special import rel_entr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_Q_new(P: np.ndarray):\n",
    "  '''\n",
    "  Compute optimal Q given 3d array P \n",
    "  with dimensions coressponding to x1, x2, and y respectively\n",
    "  '''\n",
    "  Py = P.sum(axis=0).sum(axis=0)\n",
    "  Px1 = P.sum(axis=1).sum(axis=1)\n",
    "  Px2 = P.sum(axis=0).sum(axis=1)\n",
    "  Px2y = P.sum(axis=0)\n",
    "  Px1y = P.sum(axis=1)\n",
    "  Px1y_given_x2 = P/P.sum(axis=(0,2),keepdims=True)\n",
    " \n",
    "  Q = [cp.Variable((P.shape[0], P.shape[1]), nonneg=True) for i in range(P.shape[2])]\n",
    "  Q_x1x2 = [cp.Variable((P.shape[0], P.shape[1]), nonneg=True) for i in range(P.shape[2])]\n",
    "\n",
    "  # Constraints that conditional distributions sum to 1\n",
    "  sum_to_one_Q = cp.sum([cp.sum(q) for q in Q]) == 1\n",
    "\n",
    "  # Brute force constraints # \n",
    "  # [A]: p(x1, y) == q(x1, y) \n",
    "  # [B]: p(x2, y) == q(x2, y)\n",
    "\n",
    "  # Adding [A] constraints\n",
    "  A_cstrs = []\n",
    "  for x1 in range(P.shape[0]):\n",
    "      for y in range(P.shape[2]):\n",
    "        vars = []\n",
    "        for x2 in range(P.shape[1]):\n",
    "          vars.append(Q[y][x1, x2])\n",
    "        A_cstrs.append(cp.sum(vars) == Px1y[x1,y])\n",
    "  \n",
    "  # Adding [B] constraints\n",
    "  B_cstrs = []\n",
    "  for x2 in range(P.shape[1]):\n",
    "      for y in range(P.shape[2]):\n",
    "        vars = []\n",
    "        for x1 in range(P.shape[0]):\n",
    "          vars.append(Q[y][x1, x2])\n",
    "        B_cstrs.append(cp.sum(vars) == Px2y[x2,y])\n",
    "\n",
    "  # KL divergence\n",
    "  Q_pdt_dist_cstrs = [cp.sum(Q) / P.shape[2] == Q_x1x2[i] for i in range(P.shape[2])]\n",
    "\n",
    "\n",
    "  # objective\n",
    "  obj = cp.sum([cp.sum(cp.rel_entr(Q[i], Q_x1x2[i])) for i in range(P.shape[2])])\n",
    "  # print(obj.shape)\n",
    "  all_constrs = [sum_to_one_Q] + A_cstrs + B_cstrs + Q_pdt_dist_cstrs\n",
    "  prob = cp.Problem(cp.Minimize(obj), all_constrs)\n",
    "  prob.solve(verbose=False, max_iter=50000)\n",
    "\n",
    "  # print(prob.status)\n",
    "  # print(prob.value)\n",
    "  # for j in range(P.shape[1]):\n",
    "  #  print(Q[j].value)\n",
    "\n",
    "  return np.stack([q.value for q in Q],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_binary_data(num_data):\n",
    "  # 00  0\n",
    "  # 01  0\n",
    "  # 10  0\n",
    "  # 11  1\n",
    "\n",
    "  x1 = np.random.randint(0, 2, (num_data, 1))\n",
    "  x2 = np.random.randint(0, 2, (num_data, 1))\n",
    "  data = {\n",
    "      'and': (x1, x2, 1 * np.logical_and(x1, x2)),\n",
    "      'or': (x1, x2, 1 * np.logical_or(x1, x2)),\n",
    "      'xor': (x1, x2, 1 * np.logical_xor(x1, x2)),\n",
    "      'unique1': (x1, x2, x1),\n",
    "      'redundant': (x1, x1, x1),\n",
    "      'redundant_and_unique1': (np.concatenate([x1, x2], axis=1), x2, 1 * np.logical_and(x1, x2)),\n",
    "      'redundant_or_unique1': (np.concatenate([x1, x2], axis=1), x2, 1 * np.logical_or(x1, x2)),\n",
    "      'redundant_xor_unique1': (np.concatenate([x1, x2], axis=1), x2, 1 * np.logical_xor(x1, x2)),\n",
    "  }\n",
    "  return data\n",
    "\n",
    "def convert_data_to_distribution(x1: np.ndarray, x2: np.ndarray, y: np.ndarray):\n",
    "  assert x1.size == x2.size\n",
    "  assert x1.size == y.size\n",
    "\n",
    "  numel = x1.size\n",
    "  \n",
    "  x1_discrete, x1_raw_to_discrete = extract_categorical_from_data(x1.squeeze())\n",
    "  x2_discrete, x2_raw_to_discrete = extract_categorical_from_data(x2.squeeze())\n",
    "  y_discrete, y_raw_to_discrete = extract_categorical_from_data(y.squeeze())\n",
    "\n",
    "  joint_distribution = np.zeros((len(x1_raw_to_discrete), len(x2_raw_to_discrete), len(y_raw_to_discrete)))\n",
    "  for i in range(numel):\n",
    "    joint_distribution[x1_discrete[i], x2_discrete[i], y_discrete[i]] += 1\n",
    "  joint_distribution /= np.sum(joint_distribution)\n",
    "\n",
    "  return joint_distribution, (x1_raw_to_discrete, x2_raw_to_discrete, y_raw_to_discrete)\n",
    "\n",
    "def extract_categorical_from_data(x):\n",
    "  supp = set(x)\n",
    "  raw_to_discrete = dict()\n",
    "  for i in supp:\n",
    "    raw_to_discrete[i] = len(raw_to_discrete)\n",
    "  discrete_data = [raw_to_discrete[x_] for x_ in x]\n",
    "\n",
    "  return discrete_data, raw_to_discrete \n",
    "\n",
    "def MI(P: np.ndarray):\n",
    "  ''' P has 2 dimensions '''\n",
    "  margin_1 = P.sum(axis=1)\n",
    "  margin_2 = P.sum(axis=0)\n",
    "  outer = np.outer(margin_1, margin_2)\n",
    "\n",
    "  return np.sum(rel_entr(P, outer))\n",
    "  # return np.sum(P * np.log(P/outer))\n",
    "\n",
    "def CoI(P:np.ndarray):\n",
    "  ''' P has 3 dimensions, in order X1, X2, Y '''\n",
    "  # MI(Y; X1)\n",
    "  A = P.sum(axis=1)\n",
    "\n",
    "  # MI(Y; X2)\n",
    "  B = P.sum(axis=0)\n",
    "\n",
    "  # MI(Y; (X1, X2))\n",
    "  C = P.transpose([2, 0, 1]).reshape((P.shape[2], P.shape[0]*P.shape[1]))\n",
    "\n",
    "  return MI(A) + MI(B) - MI(C)\n",
    "\n",
    "def CI(P, Q):\n",
    "  assert P.shape == Q.shape\n",
    "  P_ = P.transpose([2, 0, 1]).reshape((P.shape[2], P.shape[0]*P.shape[1]))\n",
    "  Q_ = Q.transpose([2, 0, 1]).reshape((Q.shape[2], Q.shape[0]*Q.shape[1]))\n",
    "  return MI(P_) - MI(Q_)\n",
    "\n",
    "def UI(P, cond_id=0):\n",
    "  ''' P has 3 dimensions, in order X1, X2, Y \n",
    "  We condition on X1 if cond_id = 0, if 1, then X2.\n",
    "  '''\n",
    "  sum = 0.\n",
    "\n",
    "  if cond_id == 0:\n",
    "    J= P.sum(axis=(1,2)) # marginal of x1\n",
    "    for i in range(P.shape[0]):\n",
    "      sum += MI(P[i,:,:]/P[i,:,:].sum()) * J[i]\n",
    "  elif cond_id == 1:\n",
    "    J= P.sum(axis=(0,2)) # marginal of x1\n",
    "    for i in range(P.shape[1]):\n",
    "      sum += MI(P[:,i,:]/P[:,i,:].sum()) * J[i]\n",
    "  else:\n",
    "    assert False\n",
    "\n",
    "  return sum\n",
    "\n",
    "def test(P):\n",
    "  Q = solve_Q_new(P)\n",
    "  redundancy = CoI(Q)\n",
    "  print('Redundancy', redundancy)\n",
    "  unique_1 = UI(Q, cond_id=1)\n",
    "  print('Unique', unique_1)\n",
    "  unique_2 = UI(Q, cond_id=0)\n",
    "  print('Unique', unique_2)\n",
    "  synergy = CI(P, Q)\n",
    "  print('Synergy', synergy)\n",
    "  return {'redundancy':redundancy, 'unique1':unique_1, 'unique2':unique_2, 'synergy':synergy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundancy 3.3705091871802423e-09\n",
      "Unique 1.1187109610023148e-16\n",
      "Unique 1.1187109610023148e-16\n",
      "Synergy 0.6931471771894356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'redundancy': np.float64(3.3705091871802423e-09),\n",
       " 'unique1': np.float64(1.1187109610023148e-16),\n",
       " 'unique2': np.float64(1.1187109610023148e-16),\n",
       " 'synergy': np.float64(0.6931471771894356)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.zeros((2,2,2))\n",
    "P[:,:,0] = np.eye(2) * 0.25\n",
    "P[:,:,1] = np.array([[0., 0.25], [0.25, 0.]])\n",
    "test(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.25, 0.  ],\n",
       "        [0.  , 0.25]],\n",
       "\n",
       "       [[0.  , 0.25],\n",
       "        [0.25, 0.  ]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundancy 9.98812088620106e-06\n",
      "Unique 2.2337941696321481e-10\n",
      "Unique 1.0791333733425263e-05\n",
      "Synergy 0.6931253926816073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'redundancy': np.float64(9.98812088620106e-06),\n",
       " 'unique1': np.float64(2.2337941696321481e-10),\n",
       " 'unique2': np.float64(1.0791333733425263e-05),\n",
       " 'synergy': np.float64(0.6931253926816073)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = gen_binary_data(100000)\n",
    "P, maps = convert_data_to_distribution(*data['xor'])\n",
    "test(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundancy 0.2150743943150804\n",
      "Unique 1.169119897466959e-08\n",
      "Unique 0.0004256690572395108\n",
      "Synergy 0.3458022230173096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'redundancy': np.float64(0.2150743943150804),\n",
       " 'unique1': np.float64(1.169119897466959e-08),\n",
       " 'unique2': np.float64(0.0004256690572395108),\n",
       " 'synergy': np.float64(0.3458022230173096)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = gen_binary_data(1000000)\n",
    "P, maps = convert_data_to_distribution(*data['and'])\n",
    "test(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundancy 0.005723754879699304\n",
      "Unique 0.001233640614499781\n",
      "Unique 0.028903272546529815\n",
      "Synergy 0.07845121016645373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'redundancy': np.float64(0.005723754879699304),\n",
       " 'unique1': np.float64(0.001233640614499781),\n",
       " 'unique2': np.float64(0.028903272546529815),\n",
       " 'synergy': np.float64(0.07845121016645373)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.random.uniform(size=(5,4,3))\n",
    "P = P / np.sum(P)\n",
    "test(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundancy 0.2157615011398003\n",
      "Unique 2.9101687188058173e-08\n",
      "Unique 2.9101687186610088e-08\n",
      "Synergy 0.34657358527563387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'redundancy': np.float64(0.2157615011398003),\n",
       " 'unique1': np.float64(2.9101687188058173e-08),\n",
       " 'unique2': np.float64(2.9101687186610088e-08),\n",
       " 'synergy': np.float64(0.34657358527563387)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HR_flag = np.array([1, 0, 1, 0])    # 1=HR high, 0=normal\n",
    "BP_flag = np.array([1, 1, 0, 0])    # 1=BP low, 0=normal\n",
    "Event   = np.array([1, 0, 0, 0])    # 1=shock event, 0=no event\n",
    "data = (BP_flag, HR_flag, Event)\n",
    "P, maps = convert_data_to_distribution(*data)\n",
    "test(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundancy 0.2157615011398003\n",
      "Unique 2.9101687188058173e-08\n",
      "Unique 2.9101687186610088e-08\n",
      "Synergy 0.34657358527563387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'redundancy': np.float64(0.2157615011398003),\n",
       " 'unique1': np.float64(2.9101687188058173e-08),\n",
       " 'unique2': np.float64(2.9101687186610088e-08),\n",
       " 'synergy': np.float64(0.34657358527563387)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HR_flag = np.array([1, 0, 1, 0])    # 1=HR high, 0=normal\n",
    "BP_flag = np.array([1, 1, 0, 0])    # 1=BP low, 0=normal\n",
    "Event   = np.array([1, 0, 0, 0])    # 1=shock event, 0=no event\n",
    "data = (HR_flag, BP_flag, Event)\n",
    "P, maps = convert_data_to_distribution(*data)\n",
    "test(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain structure PID: {'I(X1;Y)': 0.3093620837512343, 'I(X2;Y)': 0.5275337297323008, 'I(X1,X2;Y)': 0.5275630150785158, 'I(X1;X2;Y)': 0.30933279840501937, 'R': 0.3093620837512343, 'U1': 0.0, 'U2': 0.21817164598106653, 'S': 2.9285346214935615e-05}\n",
      "Fork structure PID:  {'I(X1;Y)': 0.323052832152251, 'I(X2;Y)': 0.5338501504233331, 'I(X1,X2;Y)': 0.5342164420634503, 'I(X1;X2;Y)': 0.32268654051213375, 'R': 0.323052832152251, 'U1': 0.0, 'U2': 0.21079731827108206, 'S': 0.00036629164011725557}\n",
      "V-structure (XOR) PID: {'I(X1;Y)': 1.3968909156059084e-05, 'I(X2;Y)': 3.5760985284127855e-09, 'I(X1,X2;Y)': 0.9998153271549208, 'I(X1;X2;Y)': -0.9998013546696662, 'R': 3.5760985284127855e-09, 'U1': 1.3965333057530671e-05, 'U2': 0.0, 'S': 0.9998013582457648}\n",
      "V-structure (OR) PID:  {'I(X1;Y)': 0.3037191732646174, 'I(X2;Y)': 0.31425939867150343, 'I(X1,X2;Y)': 0.8125436340078087, 'I(X1;X2;Y)': -0.19456506207168767, 'R': 0.3037191732646174, 'U1': 0.0, 'U2': 0.010540225406886039, 'S': 0.4982842353363053}\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "from collections import Counter\n",
    "\n",
    "# Functions to sample synthetic data for each structure\n",
    "def sample_chain(n=10000, p_noise12=0.1, p_noise2Y=0.1):\n",
    "    \"\"\"Chain: X1 -> X2 -> Y (with optional noise in X1->X2 and X2->Y links).\"\"\"\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        x1 = random.randint(0, 1)               # X1 (root cause)\n",
    "        # X2 is X1 (possibly flipped with noise probability p_noise12)\n",
    "        x2 = x1 if random.random() > p_noise12 else 1 - x1\n",
    "        # Y is X2 (possibly flipped with noise probability p_noise2Y)\n",
    "        y  = x2 if random.random() > p_noise2Y else 1 - x2\n",
    "        samples.append((x1, x2, y))\n",
    "    return samples\n",
    "\n",
    "def sample_fork(n=10000, p_noise2X1=0.1, p_noise2Y=0.1):\n",
    "    \"\"\"Fork: X2 -> X1 and X2 -> Y (X2 is common cause of X1 and Y).\"\"\"\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        x2 = random.randint(0, 1)               # X2 (common cause)\n",
    "        # X1 and Y are copies of X2 (with noise on each link)\n",
    "        x1 = x2 if random.random() > p_noise2X1 else 1 - x2\n",
    "        y  = x2 if random.random() > p_noise2Y else 1 - x2\n",
    "        samples.append((x1, x2, y))\n",
    "    return samples\n",
    "\n",
    "def sample_vstructure(n=10000, mode=\"xor\", p_noise=0.0):\n",
    "    \"\"\"\n",
    "    V-structure: X1 -> Y <- X2 with X1, X2 independent.\n",
    "    mode \"xor\": Y = X1 XOR X2 (pure synergy).\n",
    "    mode \"or\":  Y = X1 OR X2 (partial synergy).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        x1 = random.randint(0, 1)\n",
    "        x2 = random.randint(0, 1)\n",
    "        # Compute Y based on independent contributions of X1 and X2\n",
    "        if mode == \"xor\":\n",
    "            y_true = x1 ^ x2           # XOR yields Y=1 only if an odd number of inputs are 1\n",
    "        elif mode == \"or\":\n",
    "            y_true = 1 if (x1 == 1 or x2 == 1) else 0  # OR yields Y=1 if any input is 1\n",
    "        else:\n",
    "            y_true = x1 ^ x2           # default to XOR\n",
    "        # Optionally flip Y with noise\n",
    "        y = y_true if random.random() > p_noise else 1 - y_true\n",
    "        samples.append((x1, x2, y))\n",
    "    return samples\n",
    "\n",
    "# Function to compute mutual informations and PID R, U1, U2, S\n",
    "def compute_pid(measurements):\n",
    "    \"\"\"\n",
    "    Compute mutual information quantities and PID (R, U1, U2, S) \n",
    "    using the minimum mutual information heuristic for redundancy.\n",
    "    Returns a dict of computed values.\n",
    "    \"\"\"\n",
    "    N = len(measurements)\n",
    "    # Frequency counts\n",
    "    count_xyz = Counter(measurements)\n",
    "    count_x1y = Counter((x1, y) for x1, x2, y in measurements)\n",
    "    count_x2y = Counter((x2, y) for x1, x2, y in measurements)\n",
    "    count_x1  = Counter(x1 for x1, x2, y in measurements)\n",
    "    count_x2  = Counter(x2 for x1, x2, y in measurements)\n",
    "    count_y   = Counter(y  for x1, x2, y in measurements)\n",
    "    count_x1x2 = Counter((x1, x2) for x1, x2, y in measurements)\n",
    "\n",
    "    # Helper to compute entropy from a Counter\n",
    "    def entropy(counts):\n",
    "        H = 0.0\n",
    "        for _, c in counts.items():\n",
    "            p = c / N\n",
    "            if p > 0:\n",
    "                H -= p * math.log(p, 2)\n",
    "        return H\n",
    "\n",
    "    # Entropies needed for triple mutual information\n",
    "    H_x1   = entropy(count_x1)\n",
    "    H_x2   = entropy(count_x2)\n",
    "    H_y    = entropy(count_y)\n",
    "    H_x1x2 = entropy(count_x1x2)\n",
    "    H_x1y  = entropy(count_x1y)\n",
    "    H_x2y  = entropy(count_x2y)\n",
    "    H_x1x2y = entropy(count_xyz)\n",
    "\n",
    "    # Mutual informations\n",
    "    I_x1_y   = H_x1 + H_y - H_x1y                 # I(X1;Y)\n",
    "    I_x2_y   = H_x2 + H_y - H_x2y                 # I(X2;Y)\n",
    "    I_x1x2_y = H_x1x2 + H_y - H_x1x2y             # I(X1,X2;Y) \n",
    "    I_triple = H_x1 + H_x2 + H_y - H_x1x2 - H_x1y - H_x2y + H_x1x2y  # I(X1;X2;Y)\n",
    "\n",
    "    # Standard PID decomposition (Williams & Beer minimal overlap heuristic)\n",
    "    R  = min(I_x1_y, I_x2_y)            # assume redundancy = smaller individual MI\n",
    "    U1 = I_x1_y - R                    # unique info in X1\n",
    "    U2 = I_x2_y - R                    # unique info in X2\n",
    "    S  = I_x1x2_y - (R + U1 + U2)      # synergy = total info minus accounted parts\n",
    "\n",
    "    return {\n",
    "        'I(X1;Y)': I_x1_y, \n",
    "        'I(X2;Y)': I_x2_y, \n",
    "        'I(X1,X2;Y)': I_x1x2_y, \n",
    "        'I(X1;X2;Y)': I_triple, \n",
    "        'R': R, 'U1': U1, 'U2': U2, 'S': S\n",
    "    }\n",
    "\n",
    "# Generate example data for each causal structure\n",
    "random.seed(42)  # for reproducibility\n",
    "data_chain = sample_chain(n=10000, p_noise12=0.1, p_noise2Y=0.1)\n",
    "data_fork  = sample_fork(n=10000, p_noise2X1=0.1, p_noise2Y=0.1)\n",
    "data_v_xor = sample_vstructure(n=10000, mode=\"xor\", p_noise=0.0)\n",
    "data_v_or  = sample_vstructure(n=10000, mode=\"or\", p_noise=0.0)\n",
    "\n",
    "# Compute PID values for each\n",
    "pid_chain = compute_pid(data_chain)\n",
    "pid_fork  = compute_pid(data_fork)\n",
    "pid_v_xor = compute_pid(data_v_xor)\n",
    "pid_v_or  = compute_pid(data_v_or)\n",
    "\n",
    "# Print results for comparison\n",
    "print(\"Chain structure PID:\", pid_chain)\n",
    "print(\"Fork structure PID: \", pid_fork)\n",
    "print(\"V-structure (XOR) PID:\", pid_v_xor)\n",
    "print(\"V-structure (OR) PID: \", pid_v_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Chain PID: {'R': 0.3093620837512343, 'U1': 0.0, 'U2': 0.21817164598106653, 'S': 0.0}\n",
      "Adjusted Fork PID:  {'R': 0.323052832152251, 'U1': 0.0, 'U2': 0.21079731827108206, 'S': 0.0}\n",
      "Adjusted V-XOR PID: {'R': 0.0, 'U1': 1.3968909156059084e-05, 'U2': 3.5760985284127855e-09, 'S': 0.9998013582457648}\n",
      "Adjusted V-OR PID:  {'R': 0.0, 'U1': 0.3037191732646174, 'U2': 0.31425939867150343, 'S': 0.4982842353363053}\n"
     ]
    }
   ],
   "source": [
    "def adjust_pid_for_causality(pid_values):\n",
    "    \"\"\"Adjust R, S based on the sign of triple interaction information.\"\"\"\n",
    "    I3 = pid_values['I(X1;X2;Y)']\n",
    "    R, S = pid_values['R'], pid_values['S']\n",
    "    # If positive co-info (redundancy case), increase R (at least I3) and reduce S\n",
    "    if I3 > 1e-6:\n",
    "        R_adj = max(R, I3)      # ensure redundancy covers the overlap indicated by I3\n",
    "        S_adj = 0.0            # minimize synergy\n",
    "    # If negative co-info (synergy case), increase S (at least |I3|) and reduce R\n",
    "    elif I3 < -1e-6:\n",
    "        R_adj = 0.0\n",
    "        S_adj = max(S, -I3)    # ensure synergy at least the magnitude of negative I3\n",
    "    else:\n",
    "        # If I3 ~ 0, structure is ambiguous or purely additive – keep as is\n",
    "        R_adj, S_adj = R, S\n",
    "    # Recompute unique infos with adjusted R (keeping total info same)\n",
    "    I_x1y, I_x2y, I_tot = pid_values['I(X1;Y)'], pid_values['I(X2;Y)'], pid_values['I(X1,X2;Y)']\n",
    "    U1_adj = max(0.0, I_x1y - R_adj)  # cannot be negative\n",
    "    U2_adj = max(0.0, I_x2y - R_adj)\n",
    "    # (If R was lowered significantly, some info might become unique.)\n",
    "    return {'R': R_adj, 'U1': U1_adj, 'U2': U2_adj, 'S': S_adj}\n",
    "\n",
    "# Adjust PID for each scenario\n",
    "print(\"Adjusted Chain PID:\", adjust_pid_for_causality(pid_chain))\n",
    "print(\"Adjusted Fork PID: \", adjust_pid_for_causality(pid_fork))\n",
    "print(\"Adjusted V-XOR PID:\", adjust_pid_for_causality(pid_v_xor))\n",
    "print(\"Adjusted V-OR PID: \", adjust_pid_for_causality(pid_v_or))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard PID for Chain:\n",
      "{'R': 0.2604879092178548, 'U1': 0.4740577543299681, 'U2': 0, 'S': -0.2537549731827138}\n",
      "New PID for Chain:\n",
      "{'R': 0.2604879092178548, 'U1': 0.0, 'U2': np.float64(0.0), 'S': np.float64(-0.0)}\n",
      "\n",
      "Standard PID for Fork:\n",
      "{'R': 0.15018877034547537, 'U1': 0.8274647506866232, 'U2': 0, 'S': -0.8716422848647978}\n",
      "New PID for Fork:\n",
      "{'R': 0.15018877034547537, 'U1': np.float64(0.6668949196706878), 'U2': 0.0, 'S': np.float64(-0.0)}\n",
      "\n",
      "Standard PID for V-Structure:\n",
      "{'R': 0.3394560533387343, 'U1': 0, 'U2': 0.24958022751542108, 'S': 0.2623282679730138}\n",
      "New PID for V-Structure:\n",
      "{'R': 0.3394560533387343, 'U1': 0.0, 'U2': np.float64(0.24958022751542108), 'S': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "# Assume BATCH estimator is available from the document's codebase\n",
    "# Replace with actual implementation if accessible\n",
    "class BATCH:\n",
    "    def estimate(self, X1, X2, Y):\n",
    "        # Placeholder: returns standard PID values (R, U1, U2, S)\n",
    "        # In practice, use the neural network-based estimator from the document\n",
    "        I_X1_Y = self.mutual_info(X1, Y)\n",
    "        I_X2_Y = self.mutual_info(X2, Y)\n",
    "        I_X1_X2_Y = self.mutual_info(np.column_stack((X1, X2)), Y)\n",
    "        R = min(I_X1_Y, I_X2_Y)  # Simplified approximation\n",
    "        U1 = max(0, I_X1_Y - R)\n",
    "        U2 = max(0, I_X2_Y - R)\n",
    "        S = I_X1_X2_Y - I_X1_Y - I_X2_Y + R\n",
    "        return {'R': R, 'U1': U1, 'U2': U2, 'S': S}\n",
    "    \n",
    "    def mutual_info(self, X, Y):\n",
    "        # Placeholder for mutual information estimation\n",
    "        # Use histogram-based or neural estimation in practice\n",
    "        return np.random.uniform(0.1, 1.0)  # Dummy value\n",
    "\n",
    "# Data generation functions\n",
    "def generate_chain(n_samples=1000):\n",
    "    X1 = np.random.normal(0, 1, n_samples)\n",
    "    X2 = X1 + np.random.normal(0, 0.1, n_samples)  # X2 depends on X1\n",
    "    Y = X2 + np.random.normal(0, 0.1, n_samples)   # Y depends on X2\n",
    "    return X1, X2, Y\n",
    "\n",
    "def generate_fork(n_samples=1000):\n",
    "    Z = np.random.normal(0, 1, n_samples)\n",
    "    X1 = Z + np.random.normal(0, 0.1, n_samples)\n",
    "    X2 = Z + np.random.normal(0, 0.1, n_samples)\n",
    "    Y = Z + np.random.normal(0, 0.1, n_samples)\n",
    "    return X1, X2, Y\n",
    "\n",
    "def generate_v_structure(n_samples=1000):\n",
    "    X1 = np.random.normal(0, 1, n_samples)\n",
    "    X2 = np.random.normal(0, 1, n_samples)\n",
    "    Y = X1 * X2 + np.random.normal(0, 0.1, n_samples)  # Synergistic relationship\n",
    "    return X1, X2, Y\n",
    "\n",
    "# Function to estimate mutual information (simplified)\n",
    "def estimate_mi(X, Y, bins=10):\n",
    "    # Discretize for simplicity; use kernel density or neural methods in practice\n",
    "    X_discrete = np.digitize(X, np.linspace(X.min(), X.max(), bins))\n",
    "    Y_discrete = np.digitize(Y, np.linspace(Y.min(), Y.max(), bins))\n",
    "    joint = np.histogram2d(X_discrete, Y_discrete, bins=bins)[0] + 1e-10\n",
    "    px = joint.sum(axis=1) / joint.sum()\n",
    "    py = joint.sum(axis=0) / joint.sum()\n",
    "    joint /= joint.sum()\n",
    "    mi = np.sum(joint * np.log2(joint / (px[:, None] * py[None, :])))\n",
    "    return max(0, mi)\n",
    "\n",
    "# Compute new PID quantities\n",
    "def compute_new_pid(X1, X2, Y, pid):\n",
    "    epsilon = 0.01\n",
    "    # Estimate conditional mutual informations\n",
    "    I_X1_Y_given_X2 = estimate_mi(X1, Y) - estimate_mi(X2, Y)  # Approximation\n",
    "    I_X2_Y_given_X1 = estimate_mi(X2, Y) - estimate_mi(X1, Y)  # Approximation\n",
    "    I_X1_X2 = estimate_mi(X1, X2)\n",
    "    \n",
    "    # Adjust for negative values\n",
    "    I_X1_Y_given_X2 = max(0, I_X1_Y_given_X2)\n",
    "    I_X2_Y_given_X1 = max(0, I_X2_Y_given_X1)\n",
    "    \n",
    "    R_prime = pid['R']\n",
    "    U1_prime = pid['U1'] * (I_X1_Y_given_X2 / max(I_X1_Y_given_X2, epsilon))\n",
    "    U2_prime = pid['U2'] * (I_X2_Y_given_X1 / max(I_X2_Y_given_X1, epsilon))\n",
    "    S_prime = pid['S'] * (1 - I_X1_X2 / max(I_X1_X2, epsilon))\n",
    "    \n",
    "    return {'R': R_prime, 'U1': U1_prime, 'U2': U2_prime, 'S': S_prime}\n",
    "\n",
    "# Main execution\n",
    "structures = {\n",
    "    'Chain': generate_chain(),\n",
    "    'Fork': generate_fork(),\n",
    "    'V-Structure': generate_v_structure()\n",
    "}\n",
    "\n",
    "estimator = BATCH()\n",
    "\n",
    "for name, (X1, X2, Y) in structures.items():\n",
    "    # Discretize Y for BATCH (assumes discrete labels)\n",
    "    Y_discrete = KMeans(n_clusters=10).fit(Y.reshape(-1, 1)).labels_\n",
    "    \n",
    "    # Compute standard PID\n",
    "    pid_standard = estimator.estimate(X1, X2, Y_discrete)\n",
    "    print(f\"\\nStandard PID for {name}:\")\n",
    "    print(pid_standard)\n",
    "    \n",
    "    # Compute new PID\n",
    "    pid_new = compute_new_pid(X1, X2, Y, pid_standard)\n",
    "    print(f\"New PID for {name}:\")\n",
    "    print(pid_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Temporal_RUS_Guided_MoE_Architecture.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" rule for type \"image/png\" passed its test case\n",
      "       (for more information, add \"--debug=1\" on the command line)\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Temporal RUS-Guided MoE Architecture for Multimodal EHR', format='png')\n",
    "dot.attr(rankdir='TB', fontsize=\"10\")\n",
    "\n",
    "# Input modalities\n",
    "dot.node('A', 'Multimodal EHR Data', shape='box', style='filled', color='lightblue')\n",
    "dot.node('B1', 'Vital Signs', shape='box')\n",
    "dot.node('B2', 'Clinical Notes', shape='box')\n",
    "dot.node('B3', 'Medical Imaging', shape='box')\n",
    "dot.node('B4', 'ECG', shape='box')\n",
    "\n",
    "dot.edge('A', 'B1')\n",
    "dot.edge('A', 'B2')\n",
    "dot.edge('A', 'B3')\n",
    "dot.edge('A', 'B4')\n",
    "\n",
    "# Modality-specific encoders\n",
    "dot.node('C1', 'Encoder: CNN/RNN\\n(Time-Series)', shape='box')\n",
    "dot.node('C2', 'Encoder: Transformer/LSTM\\n(Text)', shape='box')\n",
    "dot.node('C3', 'Encoder: CNN/ViT\\n(Imaging)', shape='box')\n",
    "dot.node('C4', 'Encoder: 1D-CNN/Recurrent\\n(ECG)', shape='box')\n",
    "\n",
    "dot.edge('B1', 'C1')\n",
    "dot.edge('B2', 'C2')\n",
    "dot.edge('B3', 'C3')\n",
    "dot.edge('B4', 'C4')\n",
    "\n",
    "# Fusing modality representations\n",
    "dot.node('D', 'Fused Representations\\nper Time Step', shape='box', style='filled', color='lightyellow')\n",
    "dot.edge('C1', 'D')\n",
    "dot.edge('C2', 'D')\n",
    "dot.edge('C3', 'D')\n",
    "dot.edge('C4', 'D')\n",
    "\n",
    "# RUS Estimation module and its outputs\n",
    "dot.node('E', 'RUS Estimation\\nModule', shape='box', style='filled', color='lightgrey')\n",
    "dot.node('F', '(R, U, S) Scores\\n(Redundancy,\\nUniqueness, Synergy)', shape='box', style='filled', color='lightgrey')\n",
    "dot.edge('D', 'E')\n",
    "dot.edge('E', 'F')\n",
    "\n",
    "# RUS-Guided Gating module\n",
    "dot.node('G', 'RUS-Guided Gating\\nModule', shape='box', style='filled', color='orange')\n",
    "dot.edge('D', 'G')\n",
    "dot.edge('F', 'G')\n",
    "\n",
    "# Top-K Expert Selection\n",
    "dot.node('H', 'Top-K Expert\\nSelection (Sparse Routing)', shape='box', style='filled', color='palegreen')\n",
    "dot.edge('G', 'H')\n",
    "\n",
    "# Expert Modules\n",
    "dot.node('I1', 'Redundancy Experts', shape='box')\n",
    "dot.node('I2', 'Uniqueness Experts', shape='box')\n",
    "dot.node('I3', 'Synergy Experts', shape='box')\n",
    "dot.node('I4', 'Universal Experts', shape='box')\n",
    "\n",
    "dot.edge('H', 'I1')\n",
    "dot.edge('H', 'I2')\n",
    "dot.edge('H', 'I3')\n",
    "dot.edge('H', 'I4')\n",
    "\n",
    "# Aggregation of Expert Outputs\n",
    "dot.node('J', 'Expert Output\\nAggregation\\n(Weighted Fusion)', shape='box', style='filled', color='lightpink')\n",
    "dot.edge('I1', 'J')\n",
    "dot.edge('I2', 'J')\n",
    "dot.edge('I3', 'J')\n",
    "dot.edge('I4', 'J')\n",
    "\n",
    "# Temporal Integration Module\n",
    "dot.node('K', 'Temporal Integration Module\\n(Cross-Time Attention,\\nRecurrent Fusion)', shape='box', style='filled', color='wheat')\n",
    "dot.edge('J', 'K')\n",
    "\n",
    "# Prediction Head and Output\n",
    "dot.node('L', 'Prediction Head', shape='box', style='filled', color='lightblue')\n",
    "dot.edge('K', 'L')\n",
    "dot.node('M', 'Output Prediction\\n(Diagnosis/Outcome)', shape='box', style='filled', color='lightblue')\n",
    "dot.edge('L', 'M')\n",
    "\n",
    "# Loss Functions (auxiliary connections)\n",
    "dot.node('N', 'Main Task Loss\\n(Cross-Entropy/MSE)', shape='note', color='grey')\n",
    "dot.edge('M', 'N', style='dashed')\n",
    "\n",
    "dot.node('O', 'Auxiliary Losses\\n(Uniqueness, Redundancy, Synergy)', shape='note', color='grey')\n",
    "dot.edge('G', 'O', style='dashed')\n",
    "\n",
    "dot.node('P', 'Temporal Consistency Loss', shape='note', color='grey')\n",
    "dot.edge('K', 'P', style='dashed')\n",
    "\n",
    "# Optional: Batch Priority Routing\n",
    "dot.node('Q', 'Batch Priority Routing\\n(Optional)', shape='box', style='dotted', color='darkgreen')\n",
    "dot.edge('G', 'Q', style='dotted')\n",
    "dot.edge('Q', 'H', style='dotted')\n",
    "\n",
    "# Render and save diagram to a file\n",
    "dot.render('Temporal_RUS_Guided_MoE_Architecture', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from the 'diagrams' package\n",
    "# Note: You might need to install it: pip install diagrams\n",
    "# Also requires Graphviz to be installed: https://graphviz.org/download/\n",
    "from diagrams import Diagram, Cluster, Node, Edge\n",
    "\n",
    "# Define the diagram context\n",
    "# filename specifies the output file name (default is png)\n",
    "# show=False prevents the diagram from opening automatically\n",
    "with Diagram(\"TRUSMoEModel_LargeScale Architecture\", show=False, filename=\"trus_moe_large_scale_arch\", direction=\"TB\"):\n",
    "\n",
    "    # --- Input Nodes ---\n",
    "    token_emb = Node(\"Input Token Embeddings\\n(B, M, T, E_in)\")\n",
    "    rus_values = Node(\"RUS Values\\n(U, R, S)\")\n",
    "\n",
    "    # --- Processing Blocks ---\n",
    "    with Cluster(\"1. Input Processing\"):\n",
    "        input_proj = Node(\"Input Projection\\n(Linear + Scale)\")\n",
    "        flatten = Node(\"Flatten\\n(B, M*T, d_model)\")\n",
    "        pos_enc = Node(\"Positional Encoding\")\n",
    "        input_processing_group = [input_proj, flatten, pos_enc] # Group for layout\n",
    "\n",
    "    with Cluster(\"2. Encoder Stack (N Layers)\"):\n",
    "        # Represent the first block (can be either type)\n",
    "        with Cluster(\"Layer 1 (e.g., Transformer Block)\"):\n",
    "            tf_mhsa1 = Node(\"MHSA\")\n",
    "            tf_addnorm1_1 = Node(\"Add & Norm\")\n",
    "            tf_ffn1 = Node(\"FFN\")\n",
    "            tf_addnorm1_2 = Node(\"Add & Norm\")\n",
    "            transformer_block_1 = [tf_mhsa1, tf_addnorm1_1, tf_ffn1, tf_addnorm1_2]\n",
    "\n",
    "        # Represent the second block (can be the other type)\n",
    "        with Cluster(\"Layer 2 (e.g., TRUS-MoE Block)\"):\n",
    "            moe_mhsa2 = Node(\"MHSA\")\n",
    "            moe_addnorm2_1 = Node(\"Add & Norm\")\n",
    "            moe_layer2 = Node(\"TemporalRUSMoELayer\\n(RUS-Aware Router + Experts)\")\n",
    "            moe_addnorm2_2 = Node(\"Add & Norm\")\n",
    "            moe_block_2 = [moe_mhsa2, moe_addnorm2_1, moe_layer2, moe_addnorm2_2]\n",
    "\n",
    "        # Indicate repetition\n",
    "        stack_ellipsis = Node(\"...\", shape=\"plaintext\")\n",
    "\n",
    "\n",
    "    with Cluster(\"3. Output Processing\"):\n",
    "        final_norm = Node(\"Final LayerNorm\")\n",
    "        aggregate = Node(\"Aggregation\\n(e.g., Mean Pooling over Seq)\")\n",
    "        output_proj = Node(\"Output Projection\\n(Linear)\")\n",
    "        output_processing_group = [final_norm, aggregate, output_proj]\n",
    "\n",
    "    # --- Output Node ---\n",
    "    final_logits = Node(\"Final Logits\\n(B, num_classes)\")\n",
    "    aux_outputs = Node(\"MoE Aux Outputs\\n(List from MoE Layers)\") # Represent aux outputs\n",
    "\n",
    "    # --- Define Data Flow ---\n",
    "    # Input Processing\n",
    "    token_emb >> input_proj >> flatten >> pos_enc\n",
    "\n",
    "    # Into Encoder Stack\n",
    "    pos_enc >> tf_mhsa1 # Connect to the first block's input\n",
    "\n",
    "    # Flow through Transformer Block 1\n",
    "    tf_mhsa1 >> tf_addnorm1_1 >> tf_ffn1 >> tf_addnorm1_2\n",
    "\n",
    "    # Flow from Block 1 to Block 2\n",
    "    tf_addnorm1_2 >> moe_mhsa2\n",
    "\n",
    "    # Flow through TRUS-MoE Block 2\n",
    "    moe_mhsa2 >> moe_addnorm2_1 >> moe_layer2 >> moe_addnorm2_2\n",
    "    # Show RUS values feeding into the MoE layer specifically\n",
    "    rus_values >> Edge(color=\"darkgreen\", style=\"dashed\") >> moe_layer2\n",
    "    # Show Aux outputs coming from MoE layer\n",
    "    moe_layer2 >> Edge(color=\"blue\", style=\"dashed\") >> aux_outputs\n",
    "\n",
    "    # Ellipsis indicating more layers\n",
    "    moe_addnorm2_2 >> stack_ellipsis\n",
    "\n",
    "    # Out of Stack to Output Processing\n",
    "    stack_ellipsis >> final_norm # Connect from ellipsis to final processing\n",
    "\n",
    "    # Output Processing Flow\n",
    "    final_norm >> aggregate >> output_proj >> final_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dami",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
